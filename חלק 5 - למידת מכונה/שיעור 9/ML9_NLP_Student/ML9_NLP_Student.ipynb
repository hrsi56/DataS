{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h3tdBKaeZPJL",
        "7VTroFZmd4Zt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "dlkgQnhWa5z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# additional imports for the Textual features\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wy7aT1Z_WBVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ddc82e-f305-4803-c1ef-c2d44754c5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP: Bag of Words & Text Classification Tasks\n",
        "\n",
        "## The Data  \n",
        "\n",
        "We will use the **Women’s Clothing E-Commerce dataset** , which is revolving around the reviews written by customers.\n",
        "\n",
        "\n",
        "* **Review Text:** String variable for the review body.\n",
        "\n",
        "* **Recommended:** Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n"
      ],
      "metadata": {
        "id": "yGBNKVotUAmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task - \"EDA\"\n",
        "1. Load the dataset`Womens_Clothing_E-Commerce_Reviews.csv` into a pandas DataFrame.\n",
        "* You can use any other public dataset!\n",
        "2. Drop any unnecessary columns\n",
        "3. Print the number of rows and columns in the dataset.\n",
        "4. For each column, calculate:\n",
        "   - The number of **unique values**\n",
        "   - The number of **missing values**\n",
        "5. Display the result in a summary table for quick inspection.\n",
        "\n",
        " This task helps you understand the dataset structure, spot missing values, and plan preprocessing accordingly.\n"
      ],
      "metadata": {
        "id": "SocAw1yMaqAi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bihq6z5b4bbQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task - Split Train-Test\n",
        "\n",
        "1. Split your dataset into **training** and **test** sets  (80% train / 20% test)\n",
        "2. Extract the textual data from the column `'Review Text'` into two variables:\n",
        "   - `x_train_textual`\n",
        "   - `x_test_textual`\n",
        "\n",
        "2. Create two DataFrames:\n",
        "   - `train_text_df` — will hold both the raw and preprocessed review texts for the train set\n",
        "   - `test_text_df` — same for the test set\n",
        "\n",
        "Each DataFrame should have two columns:\n",
        "- `'raw text'`: the original review\n",
        "- `'preprocessed text'`: the cleaned review (to be filled in the next task)\n"
      ],
      "metadata": {
        "id": "A0v939EoZXF6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FxZeLW2MZbc1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task - Text Preprocessing\n",
        "\n",
        "Now preprocess the reviews in both `x_train_textual` and `x_test_textual`.\n",
        "\n",
        "Your preprocessing pipeline should include:\n",
        "\n",
        "- Lowercasing\n",
        "- Tokenization (by splitting on spaces)\n",
        "- Stopword removal\n",
        "- **Stemming** using NLTK’s `PorterStemmer`\n",
        "- Join the tokens back into a single string\n",
        "\n",
        "Additional instructions:\n",
        "\n",
        "- Use NLTK’s stopword list.\n",
        "- Exclude the words `\"no\"` and `\"not\"` from the stopwords list (to preserve negation).\n",
        "- Apply the pipeline separately for the train and test sets.\n",
        "- Store the results in the appropriate `'preprocessed text'` column in `train_text_df` and `test_text_df`.\n",
        "\n",
        "Feel free to use the code from the slides."
      ],
      "metadata": {
        "id": "Ip5z-Hi1322C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBHL-83g4tuJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task - Features Extraction\n",
        "\n",
        "In this task, you’ll convert the preprocessed text into numeric features using `BoW` (or `TF-IDF`).\n",
        "\n",
        "Your steps:\n",
        "\n",
        "1. Extract the `'preprocessed text'` column from both `train_text_df` and `test_text_df`, and store them in:\n",
        "   - `processed_train`\n",
        "   - `processed_test`\n",
        "\n",
        "2. Initialize a `CountVectorizer`.\n",
        "\n",
        "3. Fit the vectorizer only on the training set (`processed_train`).\n",
        "\n",
        "4. Transform both the training and test sets using the fitted vectorizer:\n",
        "   - `x_train_textual = cv.transform(processed_train)`\n",
        "   - `x_test_textual = cv.transform(processed_test)`\n",
        "\n",
        "5. Convert the results to dense numpy arrays using `.toarray()` for later compatibility with classical models.\n",
        "\n",
        "6. Print the shape and a sample of the resulting feature vectors to validate your work.\n",
        "\n"
      ],
      "metadata": {
        "id": "_a2IKSrA46om"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fto-F575XT7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task - The Model\n",
        "\n",
        "Now that you’ve preprocessed the text and converted it into numerical features, it's time to **train a classification model**.\n",
        "\n",
        "Your goal is to build a model that can **predict the target label** using the feature matrix `x_train_textual`.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Select and train a classification model of your choice (e.g.`LogisticRegression`)\n",
        "\n",
        "2. Fit the model on the **training set**.\n",
        "\n",
        "3. Use it to predict on the **test set**.\n",
        "\n",
        "4. Evaluate your model using"
      ],
      "metadata": {
        "id": "w2uI4QLwtDEd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MeD35V1iouc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}