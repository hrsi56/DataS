{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7gQFbUxOQtb"
      },
      "source": [
        "# A First Shot at Deep Learning with PyTorch\n",
        "\n",
        "In this notebook, we are going to take the first step into the world of deep learning using PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkzttrQCwaSQ"
      },
      "source": [
        "## Importing the libraries\n",
        "\n",
        "Like with any other programming exercise, the first step is to import the necessary libraries. As we are going to be using Google Colab to program our neural network, we need to install and import the necessary PyTorch libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuhJIaeXO2W9",
        "outputId": "a102bdee-7848-4c54-fd3a-303aadde3c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## The usual imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "## print out the pytorch version used\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a2C_nneO_wp"
      },
      "source": [
        "## The Neural Network\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1Lpi4VPBfAV3JkOLopcsGK4L8dyxmPF1b)\n",
        "\n",
        "Before building and training a neural network the first step is to process and prepare the data. In this notebook, we are going to use syntethic data (i.e., fake data) so we won't be using any real world data.\n",
        "\n",
        "For the sake of simplicity, we are going to use the following input and output **pairs converted to tensors**, which is how data is typically represented in the world of deep learning. The x values represent the input of dimension `(6,1)` and the y values represent the output of similar dimension.\n",
        "\n",
        "**A tensor is a multi-dimensional array that serves as the fundamental data structure in PyTorch.\n",
        "Similar to NumPy arrays but with the added ability to run on GPUs for accelerated computation.**\n",
        "\n",
        "Why Use Tensors?\n",
        "Versatility: Can represent scalars, vectors, matrices, and higher-dimensional data.\n",
        "Performance: Seamless integration with GPUs for faster computations.\n",
        "Core of Deep Learning: Tensors store input data, model parameters, and activations during training.\n",
        "\n",
        "\n",
        "The objective of the neural network model that we are going to build and train is to automatically learn patterns that better characterize the relationship between the `x` and `y` values. Essentially, the model learns the relationship that exists between inputs and outputs which can then be used to predict the corresponding `y` value for any given input `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWFtgUX85iwO"
      },
      "source": [
        "## our data in tensor form\n",
        "x = torch.tensor([[-1.0],  [0.0], [1.0], [2.0], [3.0], [4.0]], dtype=torch.float)\n",
        "y = torch.tensor([[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]], dtype=torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to(\"cuda\")\n",
        "    print(\"Tensor on GPU:\", tensor)\n",
        "else:\n",
        "    print(\"Tensor on CPU:\", tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9sKusPpfljg",
        "outputId": "021dd792-e74f-4bd1-f753-8077eccf1d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor on GPU: tensor([1., 2., 3.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcQUjR_95z5J",
        "outputId": "97fdfca1-0f89-42b9-bd82-375de20c6be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## print size of the input tensor\n",
        "x.size()\n",
        "tensor.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CJXO5WX1QtQ"
      },
      "source": [
        "## The Neural Network Components\n",
        "\n",
        "### Model\n",
        "\n",
        "* Below we show an example of how to define a hidden layer named `layer1` with size `(1, 1)`. For the purpose of this tutorial, we won't explicitly define the `weights` and allow the built-in functions provided by PyTorch to handle that part for us. By the way, the `nn.Linear(...)` function applies a linear transformation ($y = xA^T + b$) to the data that was provided as its input. We ignore the bias for now by setting `bias=False`.\n",
        "\n",
        "\n",
        "* The nn.Sequential - Wraps the defined layer1 into a Sequential model.\n",
        "A Sequential model chains multiple layers together in the order they are defined.\n",
        "Purpose:Makes it easier to add and organize additional layers if needed.\n",
        "Enables forward passes without explicitly calling the layers one by one.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Ii5JRz3Jud"
      },
      "source": [
        "## Neural network with 1 hidden layer\n",
        "layer1 = nn.Linear(1,1, bias=False)\n",
        "model = nn.Sequential(layer1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HTWYD4aMBXQ"
      },
      "source": [
        "### Loss and Optimizer\n",
        "* The loss function, `nn.MSELoss()`, is in charge of letting the model know how good it has learned the relationship between the input and output. Defines the Mean Squared Error (MSE) loss function, which calculates the average squared difference between the predicted output and the actual target.\n",
        "\n",
        "* The optimizer (in this case an `SGD`) primary role is to minimize or lower that loss value as it tunes its weights. Implements the Stochastic Gradient Descent (SGD) optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hglFpejArxx"
      },
      "source": [
        "## loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "## optimizer algorithm\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) #The learning rate determines the size of the steps taken during optimization to minimize the loss function."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKj6jvZTUtGh"
      },
      "source": [
        "## Training the Neural Network Model\n",
        "We have all the components we need to train our model. Below is the code used to train our model.\n",
        "\n",
        "In simple terms, we train the model by feeding it the input and output pairs for a couple of rounds (i.e., `epoch`). After a series of forward and backward steps, the model somewhat learns the relationship between x and y values. This is notable by the decrease in the computed `loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeOr9i-aBzRv",
        "outputId": "93f8ee06-6fcc-4efe-add9-62e0960014cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## training\n",
        "for ITER in range(150):\n",
        "    model = model.train()\n",
        "\n",
        "    ## forward\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## backward + update model params\n",
        "    loss.backward() #Computes the gradients of the loss with respect to each model parameter (weights and biases) using backpropagation.\n",
        "    optimizer.step() #Updates the model parameters based on the computed gradients.\n",
        "\n",
        "    model.eval()\n",
        "    print('Epoch: %d | Loss: %.4f' %(ITER, loss.detach().item())) #Converts the PyTorch tensor loss to a Python scalar for display."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 38.3069\n",
            "Epoch: 1 | Loss: 30.9098\n",
            "Epoch: 2 | Loss: 24.9625\n",
            "Epoch: 3 | Loss: 20.1808\n",
            "Epoch: 4 | Loss: 16.3362\n",
            "Epoch: 5 | Loss: 13.2451\n",
            "Epoch: 6 | Loss: 10.7599\n",
            "Epoch: 7 | Loss: 8.7617\n",
            "Epoch: 8 | Loss: 7.1551\n",
            "Epoch: 9 | Loss: 5.8634\n",
            "Epoch: 10 | Loss: 4.8249\n",
            "Epoch: 11 | Loss: 3.9899\n",
            "Epoch: 12 | Loss: 3.3186\n",
            "Epoch: 13 | Loss: 2.7788\n",
            "Epoch: 14 | Loss: 2.3448\n",
            "Epoch: 15 | Loss: 1.9959\n",
            "Epoch: 16 | Loss: 1.7154\n",
            "Epoch: 17 | Loss: 1.4898\n",
            "Epoch: 18 | Loss: 1.3085\n",
            "Epoch: 19 | Loss: 1.1627\n",
            "Epoch: 20 | Loss: 1.0454\n",
            "Epoch: 21 | Loss: 0.9512\n",
            "Epoch: 22 | Loss: 0.8754\n",
            "Epoch: 23 | Loss: 0.8145\n",
            "Epoch: 24 | Loss: 0.7655\n",
            "Epoch: 25 | Loss: 0.7261\n",
            "Epoch: 26 | Loss: 0.6944\n",
            "Epoch: 27 | Loss: 0.6690\n",
            "Epoch: 28 | Loss: 0.6485\n",
            "Epoch: 29 | Loss: 0.6320\n",
            "Epoch: 30 | Loss: 0.6188\n",
            "Epoch: 31 | Loss: 0.6082\n",
            "Epoch: 32 | Loss: 0.5996\n",
            "Epoch: 33 | Loss: 0.5927\n",
            "Epoch: 34 | Loss: 0.5872\n",
            "Epoch: 35 | Loss: 0.5828\n",
            "Epoch: 36 | Loss: 0.5792\n",
            "Epoch: 37 | Loss: 0.5763\n",
            "Epoch: 38 | Loss: 0.5740\n",
            "Epoch: 39 | Loss: 0.5721\n",
            "Epoch: 40 | Loss: 0.5706\n",
            "Epoch: 41 | Loss: 0.5694\n",
            "Epoch: 42 | Loss: 0.5685\n",
            "Epoch: 43 | Loss: 0.5677\n",
            "Epoch: 44 | Loss: 0.5671\n",
            "Epoch: 45 | Loss: 0.5666\n",
            "Epoch: 46 | Loss: 0.5662\n",
            "Epoch: 47 | Loss: 0.5658\n",
            "Epoch: 48 | Loss: 0.5656\n",
            "Epoch: 49 | Loss: 0.5654\n",
            "Epoch: 50 | Loss: 0.5652\n",
            "Epoch: 51 | Loss: 0.5651\n",
            "Epoch: 52 | Loss: 0.5650\n",
            "Epoch: 53 | Loss: 0.5649\n",
            "Epoch: 54 | Loss: 0.5648\n",
            "Epoch: 55 | Loss: 0.5647\n",
            "Epoch: 56 | Loss: 0.5647\n",
            "Epoch: 57 | Loss: 0.5647\n",
            "Epoch: 58 | Loss: 0.5646\n",
            "Epoch: 59 | Loss: 0.5646\n",
            "Epoch: 60 | Loss: 0.5646\n",
            "Epoch: 61 | Loss: 0.5646\n",
            "Epoch: 62 | Loss: 0.5646\n",
            "Epoch: 63 | Loss: 0.5646\n",
            "Epoch: 64 | Loss: 0.5645\n",
            "Epoch: 65 | Loss: 0.5645\n",
            "Epoch: 66 | Loss: 0.5645\n",
            "Epoch: 67 | Loss: 0.5645\n",
            "Epoch: 68 | Loss: 0.5645\n",
            "Epoch: 69 | Loss: 0.5645\n",
            "Epoch: 70 | Loss: 0.5645\n",
            "Epoch: 71 | Loss: 0.5645\n",
            "Epoch: 72 | Loss: 0.5645\n",
            "Epoch: 73 | Loss: 0.5645\n",
            "Epoch: 74 | Loss: 0.5645\n",
            "Epoch: 75 | Loss: 0.5645\n",
            "Epoch: 76 | Loss: 0.5645\n",
            "Epoch: 77 | Loss: 0.5645\n",
            "Epoch: 78 | Loss: 0.5645\n",
            "Epoch: 79 | Loss: 0.5645\n",
            "Epoch: 80 | Loss: 0.5645\n",
            "Epoch: 81 | Loss: 0.5645\n",
            "Epoch: 82 | Loss: 0.5645\n",
            "Epoch: 83 | Loss: 0.5645\n",
            "Epoch: 84 | Loss: 0.5645\n",
            "Epoch: 85 | Loss: 0.5645\n",
            "Epoch: 86 | Loss: 0.5645\n",
            "Epoch: 87 | Loss: 0.5645\n",
            "Epoch: 88 | Loss: 0.5645\n",
            "Epoch: 89 | Loss: 0.5645\n",
            "Epoch: 90 | Loss: 0.5645\n",
            "Epoch: 91 | Loss: 0.5645\n",
            "Epoch: 92 | Loss: 0.5645\n",
            "Epoch: 93 | Loss: 0.5645\n",
            "Epoch: 94 | Loss: 0.5645\n",
            "Epoch: 95 | Loss: 0.5645\n",
            "Epoch: 96 | Loss: 0.5645\n",
            "Epoch: 97 | Loss: 0.5645\n",
            "Epoch: 98 | Loss: 0.5645\n",
            "Epoch: 99 | Loss: 0.5645\n",
            "Epoch: 100 | Loss: 0.5645\n",
            "Epoch: 101 | Loss: 0.5645\n",
            "Epoch: 102 | Loss: 0.5645\n",
            "Epoch: 103 | Loss: 0.5645\n",
            "Epoch: 104 | Loss: 0.5645\n",
            "Epoch: 105 | Loss: 0.5645\n",
            "Epoch: 106 | Loss: 0.5645\n",
            "Epoch: 107 | Loss: 0.5645\n",
            "Epoch: 108 | Loss: 0.5645\n",
            "Epoch: 109 | Loss: 0.5645\n",
            "Epoch: 110 | Loss: 0.5645\n",
            "Epoch: 111 | Loss: 0.5645\n",
            "Epoch: 112 | Loss: 0.5645\n",
            "Epoch: 113 | Loss: 0.5645\n",
            "Epoch: 114 | Loss: 0.5645\n",
            "Epoch: 115 | Loss: 0.5645\n",
            "Epoch: 116 | Loss: 0.5645\n",
            "Epoch: 117 | Loss: 0.5645\n",
            "Epoch: 118 | Loss: 0.5645\n",
            "Epoch: 119 | Loss: 0.5645\n",
            "Epoch: 120 | Loss: 0.5645\n",
            "Epoch: 121 | Loss: 0.5645\n",
            "Epoch: 122 | Loss: 0.5645\n",
            "Epoch: 123 | Loss: 0.5645\n",
            "Epoch: 124 | Loss: 0.5645\n",
            "Epoch: 125 | Loss: 0.5645\n",
            "Epoch: 126 | Loss: 0.5645\n",
            "Epoch: 127 | Loss: 0.5645\n",
            "Epoch: 128 | Loss: 0.5645\n",
            "Epoch: 129 | Loss: 0.5645\n",
            "Epoch: 130 | Loss: 0.5645\n",
            "Epoch: 131 | Loss: 0.5645\n",
            "Epoch: 132 | Loss: 0.5645\n",
            "Epoch: 133 | Loss: 0.5645\n",
            "Epoch: 134 | Loss: 0.5645\n",
            "Epoch: 135 | Loss: 0.5645\n",
            "Epoch: 136 | Loss: 0.5645\n",
            "Epoch: 137 | Loss: 0.5645\n",
            "Epoch: 138 | Loss: 0.5645\n",
            "Epoch: 139 | Loss: 0.5645\n",
            "Epoch: 140 | Loss: 0.5645\n",
            "Epoch: 141 | Loss: 0.5645\n",
            "Epoch: 142 | Loss: 0.5645\n",
            "Epoch: 143 | Loss: 0.5645\n",
            "Epoch: 144 | Loss: 0.5645\n",
            "Epoch: 145 | Loss: 0.5645\n",
            "Epoch: 146 | Loss: 0.5645\n",
            "Epoch: 147 | Loss: 0.5645\n",
            "Epoch: 148 | Loss: 0.5645\n",
            "Epoch: 149 | Loss: 0.5645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp50Q7J0Xkiw"
      },
      "source": [
        "## Testing the Model\n",
        "After training the model we have the ability to test the model predictive capability by passing it an input. Below is a simple example of how you could achieve this with our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1odfZpGFoBi",
        "outputId": "452b7b9b-6162-406e-db3a-dcfb86cdb005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## test the model\n",
        "sample = torch.tensor([10.0], dtype=torch.float)\n",
        "predicted = model(sample)\n",
        "print(predicted.detach().item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.096769332885742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Exploring Tensor Basics##\n",
        "\n",
        "Exercise:\n",
        "\n",
        "**Part 1**: Basic Operations\n",
        "Create a 2D tensor of random values with size (3, 3).\n",
        "Perform the following:\n",
        "- Transpose the tensor.\n",
        "- Compute its mean and standard deviation.\n",
        "- Move the tensor to a GPU (if available).\n",
        "\n",
        "**Part 2**: Arithmetic Operations\n",
        "Create another random tensor of the same size (3, 3) and:\n",
        "- Add the two tensors.\n",
        "- Multiply them element-wise.\n",
        "\n",
        "**Part 3**: Reshaping and Broadcasting\n",
        "- Reshape one of the tensors to (9,) and back to (3, 3).\n",
        "\n",
        "**Part 4**: Advanced Tensor Indexing\n",
        "Perform slicing to extract:\n",
        "- The first row.\n",
        "- The last column.\n",
        "- All elements greater than a certain value (e.g., 0.5).\n"
      ],
      "metadata": {
        "id": "o3v5EeY5nXWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Part 1: Basic Operations\n",
        "# Create a 2D tensor\n",
        "tensor = torch.rand(3, 3)\n",
        "print(\"Original Tensor:\\n\", tensor)\n",
        "\n",
        "# Transpose the tensor\n",
        "tensor_t = tensor.T\n",
        "print(\"Transposed Tensor:\\n\", tensor_t)\n",
        "\n",
        "# Compute mean and standard deviation\n",
        "mean = tensor.mean()\n",
        "std = tensor.std()\n",
        "print(\"Mean:\", mean.item(), \"Std Dev:\", std.item())\n",
        "\n",
        "# Move to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    tensor_gpu = tensor.to(\"cuda\")\n",
        "    print(\"Tensor on GPU:\\n\", tensor_gpu)\n",
        "\n",
        "# Part 2: Arithmetic Operations\n",
        "# Create another random tensor\n",
        "tensor2 = torch.rand(3, 3)\n",
        "\n",
        "# Element-wise addition\n",
        "sum_tensor = tensor + tensor2\n",
        "print(\"Element-wise Addition:\\n\", sum_tensor)\n",
        "\n",
        "# Element-wise multiplication\n",
        "product_tensor = tensor * tensor2\n",
        "print(\"Element-wise Multiplication:\\n\", product_tensor)\n",
        "\n",
        "# Part 3: Reshaping\n",
        "# Reshape tensor\n",
        "reshaped_tensor = tensor.view(9)\n",
        "print(\"Reshaped Tensor:\\n\", reshaped_tensor)\n",
        "\n",
        "# Reshape back to original\n",
        "reshaped_back_tensor = reshaped_tensor.view(3, 3)\n",
        "print(\"Reshaped Back Tensor:\\n\", reshaped_back_tensor)\n",
        "\n",
        "# Part 4: Advanced Tensor Indexing\n",
        "# Extract specific elements\n",
        "first_row = tensor[0, :]\n",
        "print(\"First Row:\\n\", first_row)\n",
        "\n",
        "last_column = tensor[:, -1]\n",
        "print(\"Last Column:\\n\", last_column)\n",
        "\n",
        "elements_greater_than_05 = tensor[tensor > 0.5]\n",
        "print(\"Elements Greater Than 0.5:\\n\", elements_greater_than_05)\n"
      ],
      "metadata": {
        "id": "2uEHAjyvvAMy",
        "outputId": "77ff7a20-321b-411c-ed76-426ea2bab060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor:\n",
            " tensor([[0.7126, 0.7316, 0.5398],\n",
            "        [0.5471, 0.3772, 0.1180],\n",
            "        [0.8992, 0.2857, 0.0736]])\n",
            "Transposed Tensor:\n",
            " tensor([[0.7126, 0.5471, 0.8992],\n",
            "        [0.7316, 0.3772, 0.2857],\n",
            "        [0.5398, 0.1180, 0.0736]])\n",
            "Mean: 0.47609981894493103 Std Dev: 0.28407466411590576\n",
            "Tensor on GPU:\n",
            " tensor([[0.7126, 0.7316, 0.5398],\n",
            "        [0.5471, 0.3772, 0.1180],\n",
            "        [0.8992, 0.2857, 0.0736]], device='cuda:0')\n",
            "Element-wise Addition:\n",
            " tensor([[1.0868, 1.7132, 1.0443],\n",
            "        [0.5667, 0.3821, 0.2061],\n",
            "        [1.4185, 0.5656, 1.0166]])\n",
            "Element-wise Multiplication:\n",
            " tensor([[0.2667, 0.7181, 0.2723],\n",
            "        [0.0107, 0.0019, 0.0104],\n",
            "        [0.4669, 0.0800, 0.0694]])\n",
            "Reshaped Tensor:\n",
            " tensor([0.7126, 0.7316, 0.5398, 0.5471, 0.3772, 0.1180, 0.8992, 0.2857, 0.0736])\n",
            "Reshaped Back Tensor:\n",
            " tensor([[0.7126, 0.7316, 0.5398],\n",
            "        [0.5471, 0.3772, 0.1180],\n",
            "        [0.8992, 0.2857, 0.0736]])\n",
            "First Row:\n",
            " tensor([0.7126, 0.7316, 0.5398])\n",
            "Last Column:\n",
            " tensor([0.5398, 0.1180, 0.0736])\n",
            "Elements Greater Than 0.5:\n",
            " tensor([0.7126, 0.7316, 0.5398, 0.5471, 0.8992])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random 2D tensor\n",
        "tensor = torch.rand(3, 3)\n",
        "\n",
        "# Transpose the tensor\n",
        "tensor_t = tensor.T\n",
        "\n",
        "# Compute mean and standard deviation\n",
        "mean = tensor.mean()\n",
        "std = tensor.std()\n",
        "\n",
        "# Move to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    tensor_gpu = tensor.to(\"cuda\")\n",
        "    print(\"Tensor on GPU:\", tensor_gpu)\n",
        "\n",
        "print(\"Original Tensor:\", tensor)\n",
        "print(\"Transposed Tensor:\", tensor_t)\n",
        "print(\"Mean:\", mean.item(), \"Std Dev:\", std.item())\n"
      ],
      "metadata": {
        "id": "gkg65UCanhBF",
        "outputId": "7be3a804-223e-4b8e-8999-8eff85b3983e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor on GPU: tensor([[0.8545, 0.5221, 0.1670],\n",
            "        [0.9671, 0.0054, 0.8206],\n",
            "        [0.5487, 0.8343, 0.3922]], device='cuda:0')\n",
            "Original Tensor: tensor([[0.8545, 0.5221, 0.1670],\n",
            "        [0.9671, 0.0054, 0.8206],\n",
            "        [0.5487, 0.8343, 0.3922]])\n",
            "Transposed Tensor: tensor([[0.8545, 0.9671, 0.5487],\n",
            "        [0.5221, 0.0054, 0.8343],\n",
            "        [0.1670, 0.8206, 0.3922]])\n",
            "Mean: 0.5679792165756226 Std Dev: 0.33296021819114685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural Network Architecture ##\n",
        "Exercise:\n",
        "\n",
        "Part 1: Add Multiple Hidden Layers\n",
        "Modify the current network to include multiple hidden layers.\n",
        "Use nn.Sequential to define the model with at least 3 hidden layers.\n",
        "\n",
        "Part 2: Experiment with Activation Functions\n",
        "Insert different activation functions (e.g., ReLU, Tanh, Sigmoid) between the layers.\n",
        "Compare the performance of the model with each activation function.\n",
        "\n",
        "Part 3: Adjust the Size of Hidden Layers\n",
        "Vary the size (number of neurons) in each hidden layer:\n",
        "Example: Try [10, 20, 10] neurons across three layers.\n",
        "Experiment with smaller or larger layer sizes.\n",
        "\n",
        "compare between all the architectures"
      ],
      "metadata": {
        "id": "tKQ2uVgwnr13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a function to create and train a model\n",
        "def create_and_train_model(hidden_layers, activations, input_size=1, output_size=1, epochs=150, lr=0.01):\n",
        "    # Build the model dynamically\n",
        "    layers = []\n",
        "    in_features = input_size\n",
        "    for i, hidden_size in enumerate(hidden_layers):\n",
        "        layers.append(nn.Linear(in_features, hidden_size))\n",
        "        layers.append(activations[i])  # Add activation function\n",
        "        in_features = hidden_size\n",
        "    layers.append(nn.Linear(in_features, output_size))  # Output layer\n",
        "    model = nn.Sequential(*layers)\n",
        "\n",
        "    # Optimizer and criterion\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train the model\n",
        "    for ITER in range(epochs):\n",
        "        model.train()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model, loss.item()\n",
        "\n",
        "# Experiment with various architectures\n",
        "architectures = [\n",
        "    {\"hidden_layers\": [10], \"activations\": [nn.ReLU()]},\n",
        "    {\"hidden_layers\": [10, 20, 10], \"activations\": [nn.ReLU(), nn.ReLU(), nn.ReLU()]},\n",
        "    {\"hidden_layers\": [15, 15], \"activations\": [nn.Tanh(), nn.Tanh()]},\n",
        "    {\"hidden_layers\": [10, 10, 10], \"activations\": [nn.Sigmoid(), nn.Sigmoid(), nn.Sigmoid()]}\n",
        "]\n",
        "\n",
        "results = []\n",
        "for arch in architectures:\n",
        "    model, loss = create_and_train_model(arch[\"hidden_layers\"], arch[\"activations\"])\n",
        "    results.append({\"Architecture\": arch[\"hidden_layers\"], \"Activation\": arch[\"activations\"], \"Loss\": loss})\n",
        "\n",
        "# Print results\n",
        "for res in results:\n",
        "    print(f\"Architecture: {res['Architecture']}, Activations: {[type(act).__name__ for act in res['Activation']]}, Loss: {res['Loss']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "VMogn8e1niVd",
        "outputId": "9004c620-0455-4fa9-860d-41cd63794239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture: [10], Activations: ['ReLU'], Loss: 0.022010646760463715\n",
            "Architecture: [10, 20, 10], Activations: ['ReLU', 'ReLU', 'ReLU'], Loss: 0.06420841068029404\n",
            "Architecture: [15, 15], Activations: ['Tanh', 'Tanh'], Loss: 0.10088322311639786\n",
            "Architecture: [10, 10, 10], Activations: ['Sigmoid', 'Sigmoid', 'Sigmoid'], Loss: 11.584339141845703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loss Function Experimentation ##\n",
        "\n",
        "Choose 3 different loss functions to experiment with (nn.MSELoss(), nn.L1Loss()...)\n",
        "\n",
        "Train the model with each loss function using the same training data.\n",
        "Record the final loss for each loss function after 50 epochs.\n",
        "Compare the results:\n",
        "Print the final loss for each loss function.\n",
        "Summarize how the choice of loss function impacts the model's learning."
      ],
      "metadata": {
        "id": "9utCtBuYn62d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of loss functions to test\n",
        "loss_functions = {\n",
        "    \"MSELoss\": nn.MSELoss(),\n",
        "    \"L1Loss\": nn.L1Loss(),\n",
        "    \"SmoothL1Loss\": nn.SmoothL1Loss(),\n",
        "    \"HingeEmbeddingLoss\": nn.HingeEmbeddingLoss(),\n",
        "    \"HuberLoss\": nn.HuberLoss()\n",
        "}\n",
        "\n",
        "final_losses = []\n",
        "\n",
        "# Loop through each loss function\n",
        "for loss_name, criterion in loss_functions.items():\n",
        "    print(f\"\\nTesting Loss Function: {loss_name}\")\n",
        "\n",
        "    # Reinitialize the model and optimizer for each experiment\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(1, 1, bias=False)\n",
        "    )\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Train the model\n",
        "    for ITER in range(150):\n",
        "        model.train()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Record the final loss\n",
        "    final_losses.append((loss_name, loss.detach().item()))\n",
        "    print(f\"Final Loss for {loss_name}: {loss.detach().item()}\")\n",
        "\n",
        "# Print all results\n",
        "print(\"\\nLoss Function Results:\")\n",
        "for loss_name, loss in final_losses:\n",
        "    print(f\"Loss Function: {loss_name}, Final Loss: {loss}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "U24v8A4NoO0U",
        "outputId": "c37b993c-c8a1-49e2-f6c1-aa89781001e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Loss Function: MSELoss\n",
            "Final Loss for MSELoss: 0.5645161271095276\n",
            "\n",
            "Testing Loss Function: L1Loss\n",
            "Final Loss for L1Loss: 0.6113112568855286\n",
            "\n",
            "Testing Loss Function: SmoothL1Loss\n",
            "Final Loss for SmoothL1Loss: 0.2750014364719391\n",
            "\n",
            "Testing Loss Function: HingeEmbeddingLoss\n",
            "Final Loss for HingeEmbeddingLoss: 0.8345963358879089\n",
            "\n",
            "Testing Loss Function: HuberLoss\n",
            "Final Loss for HuberLoss: 0.27561065554618835\n",
            "\n",
            "Loss Function Results:\n",
            "Loss Function: MSELoss, Final Loss: 0.5645161271095276\n",
            "Loss Function: L1Loss, Final Loss: 0.6113112568855286\n",
            "Loss Function: SmoothL1Loss, Final Loss: 0.2750014364719391\n",
            "Loss Function: HingeEmbeddingLoss, Final Loss: 0.8345963358879089\n",
            "Loss Function: HuberLoss, Final Loss: 0.27561065554618835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate Exploration ##\n",
        "Exercise:\n",
        "\n",
        "Select 5 different learning rates (e.g., 0.1, 0.01, 0.001, 0.0001, 0.5).\n",
        "For each learning rate:\n",
        "1. Train the model using the same training data.\n",
        "2. Record the loss after 50 epochs.\n",
        "3. Compare the results for all learning rates:\n",
        "Print the final loss for each learning rate.\n",
        "Observe and explain how the learning rate affects convergence.\n"
      ],
      "metadata": {
        "id": "jT7lJyQxojwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of learning rates to test\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.2]\n",
        "final_losses = []\n",
        "\n",
        "# Loop through each learning rate\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTesting learning rate: {lr}\")\n",
        "\n",
        "    # Reinitialize the model and optimizer for each experiment\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(1, 1, bias=False)\n",
        "    )\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train the model\n",
        "    for ITER in range(150):\n",
        "        model.train()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Record the final loss\n",
        "    final_losses.append((lr, loss.detach().item()))\n",
        "    print(f\"Final Loss for learning rate {lr}: {loss.detach().item()}\")\n",
        "\n",
        "# Print all results\n",
        "print(\"\\nLearning Rate Results:\")\n",
        "for lr, loss in final_losses:\n",
        "    print(f\"Learning Rate: {lr}, Final Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "zHIwOEYIomQK",
        "outputId": "cfea34d7-b706-40b6-af8c-23f6359ccbda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing learning rate: 0.1\n",
            "Final Loss for learning rate 0.1: 0.5645161271095276\n",
            "\n",
            "Testing learning rate: 0.01\n",
            "Final Loss for learning rate 0.01: 0.5645161271095276\n",
            "\n",
            "Testing learning rate: 0.001\n",
            "Final Loss for learning rate 0.001: 1.4162936210632324\n",
            "\n",
            "Testing learning rate: 0.0001\n",
            "Final Loss for learning rate 0.0001: 16.821443557739258\n",
            "\n",
            "Testing learning rate: 0.2\n",
            "Final Loss for learning rate 0.2: 1704851456.0\n",
            "\n",
            "Learning Rate Results:\n",
            "Learning Rate: 0.1, Final Loss: 0.5645161271095276\n",
            "Learning Rate: 0.01, Final Loss: 0.5645161271095276\n",
            "Learning Rate: 0.001, Final Loss: 1.4162936210632324\n",
            "Learning Rate: 0.0001, Final Loss: 16.821443557739258\n",
            "Learning Rate: 0.2, Final Loss: 1704851456.0\n"
          ]
        }
      ]
    }
  ]
}